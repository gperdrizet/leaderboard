{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eaa8054",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [1]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf4ac1",
   "metadata": {
    "papermill": {
     "duration": 0.003786,
     "end_time": "2025-11-20T18:30:59.862539",
     "exception": false,
     "start_time": "2025-11-20T18:30:59.858753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Notebook set up\n",
    "\n",
    "Submit your notebook to the class leaderboard on HuggingFace at [huggingface.co/spaces/gperdrizet/leaderboard](https://huggingface.co/spaces/gperdrizet/leaderboard)\n",
    "\n",
    "**Your task**: Apply at least two different feature engineering techniques to the `housing_df` dataframe to improve the dataset. At the end of the notebook, your engineered dataset and the original dataset will be used to train a linear regression model to predict `MedHouseVal`. Your goal is to achieve better model performance via feature engineering.\n",
    "\n",
    "Don't change any of the code in the Model evaluation section of the notebook, especially the output saving. Otherwise the leaderboard scoring may not work!\n",
    "\n",
    "**Note**: If you have read ahead or you are familiar with the basics of training ML models, no there is no train-test split and yes, this means data leakage/genralizability is a concern. We will cover those topics in the next unit. For now, the goal is to keep things simple while still giving you an idea of how your feature engineering effects model performance.\n",
    "\n",
    "Before applying transformations, explore the dataset to understand what techniques would be most beneficial.\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac752b",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6be57799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T18:30:59.871031Z",
     "iopub.status.busy": "2025-11-20T18:30:59.870787Z",
     "iopub.status.idle": "2025-11-20T18:31:00.735511Z",
     "shell.execute_reply": "2025-11-20T18:31:00.734567Z"
    },
    "papermill": {
     "duration": 0.87064,
     "end_time": "2025-11-20T18:31:00.736789",
     "exception": true,
     "start_time": "2025-11-20T18:30:59.866149",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import (\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    MinMaxScaler,\n",
    "    StandardScaler,\n",
    "    OrdinalEncoder,\n",
    "    OneHotEncoder\n",
    ")\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(315)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f01df0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857336f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load California housing dataset\n",
    "original_housing_df = pd.read_csv('https://gperdrizet.github.io/FSA_devops/assets/data/unit2/california_housing.csv')\n",
    "housing_df = original_housing_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b3f456",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Task 1: Explore the dataset\n",
    "\n",
    "Before deciding what feature engineering techniques to apply, explore the dataset to understand its characteristics.\n",
    "\n",
    "**Things to investigate**:\n",
    "- Display basic information about the dataset (`.info()`, `.describe()`)\n",
    "- Check for missing values\n",
    "- Examine feature distributions (histograms, box plots)\n",
    "- Look at feature scales and ranges\n",
    "\n",
    "Use this exploration to inform your feature engineering decisions in the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda2f56",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Display basic information about the dataset (`.info()`, `.describe()`)\n",
    "print(housing_df.info())\n",
    "print(housing_df.describe())\n",
    "#Check for missing values\n",
    "print(housing_df.isnull())\n",
    "print(housing_df.isnull().sum()) # Dont have any missing values\n",
    "#Examine feature distributions (histograms, box plots)\n",
    "\n",
    "# Draw Histograms \n",
    "fig, axes = plt.subplots(4, 2, figsize=(15,20)) # 2 rows, 2 columns\n",
    "\n",
    "\n",
    "# MedInc - Distribution\n",
    "sns.histplot(housing_df['MedInc'], kde=True, color='gray', label='Original', ax=axes[0,0],alpha=0.5)\n",
    "axes[0,0].set_title('MedInc - Distribution')\n",
    "axes[0,0].set_xlabel('MedInc')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].legend()\n",
    "# HouseAge - Distribution\n",
    "sns.histplot(housing_df['HouseAge'], kde=True, color='green', label='Original', ax=axes[0,1],alpha=0.5)\n",
    "axes[0,1].set_title('HouseAge - Distribution')\n",
    "axes[0,1].set_xlabel('HouseAge')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].legend()\n",
    "# AveRooms  - Distribution\n",
    "sns.histplot(housing_df['AveRooms'], kde=True,color='pink', label='Original', ax=axes[1,0],alpha=0.5)\n",
    "axes[1,0].set_title('AveRooms - Distribution')\n",
    "axes[1,0].set_xlabel('AveRooms')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].legend()\n",
    "# AveBedrms - Distribution\n",
    "sns.histplot(housing_df['AveBedrms'], kde=True, color='blue', label='Original', ax=axes[1,1],alpha=0.5)\n",
    "axes[1,1].set_title('AveBedrms - Distribution')\n",
    "axes[1,1].set_xlabel('AveBedrms ')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].legend()\n",
    "# Population - Distribution\n",
    "sns.histplot(housing_df['Population'], kde=True, color='red', label='Original', ax=axes[2,0],alpha=0.5)\n",
    "axes[2,0].set_title('Population - Distribution')\n",
    "axes[2,0].set_xlabel('Population')\n",
    "axes[2,0].set_ylabel('Frequency')\n",
    "axes[2,0].legend()\n",
    "\n",
    "# AveOccup - Distribution\n",
    "sns.histplot(housing_df['AveOccup'], kde=True, color='red', label='Original', ax=axes[2,1],alpha=0.5)\n",
    "axes[2,1].set_title('AveOccup - Distribution')\n",
    "axes[2,1].set_xlabel('AveOccup')\n",
    "axes[2,1].set_ylabel('Frequency')\n",
    "axes[2,1].legend()\n",
    "\n",
    "# MedHouseVal - Distribution\n",
    "sns.histplot(housing_df['MedHouseVal'], kde=True, color='lightBlue', label='Original', ax=axes[3,0],alpha=0.5)\n",
    "axes[3,0].set_title('MedHouseVal - Distribution')\n",
    "axes[3,0].set_xlabel('MedHouseVal')\n",
    "axes[3,0].set_ylabel('Frequency')\n",
    "axes[3,0].legend()\n",
    "\n",
    "# Longitude adn Latitude - Distribution\n",
    "sns.scatterplot(x=housing_df['Longitude'],y=housing_df['Latitude'], label='Original', ax=axes[3,1],alpha=0.7)\n",
    "axes[3,1].set_title('Longitude - Latitude - Distribution')\n",
    "axes[3,1].set_xlabel('Longitude')\n",
    "axes[3,1].set_ylabel('Latitude')\n",
    "axes[3,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4399a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Task 2: Apply your first feature engineering technique\n",
    "\n",
    "Based on your exploration, apply your first feature engineering technique.\n",
    "\n",
    "**Example approaches**:\n",
    "- Transform skewed features using log, sqrt, power, or quantile transformations\n",
    "- Create bins/categories from continuous variables\n",
    "- Create interaction features (e.g., rooms per household = total rooms / households)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140a3a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Applying log,sqrt,power or quantile on heavily skewed features AveOccup, AveBedrms and AveRooms\n",
    "\n",
    "# Transform AveOccup\n",
    "feature = 'AveOccup'\n",
    "\n",
    "# Apply log transformation (using log1p to handle zeros)\n",
    "housing_df[f'{feature}_log'] = np.log1p(housing_df[feature])\n",
    "# Apply square root transformation\n",
    "housing_df[f'{feature}_sqrt'] = np.sqrt(housing_df[feature])\n",
    "# Apply power transformation using Yeo-Johnson (handles positive and negative values)\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "# Fit and transform\n",
    "housing_df[f'{feature}_power'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to uniform distribution\n",
    "transformer = QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "housing_df[f'{feature}_quantile_uniform'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to normal distribution\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "housing_df[f'{feature}_quantile_normal'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 4))\n",
    "\n",
    "axes[0].set_title('Original distribution')\n",
    "axes[0].hist(housing_df[feature], bins=50, edgecolor='black', color='grey')\n",
    "axes[0].set_xlabel(feature)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].set_title('Log-transformed distribution')\n",
    "axes[1].hist(housing_df[f'{feature}_log'], bins=50, edgecolor='black', color='grey')\n",
    "axes[1].set_xlabel(f'{feature}_log')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].set_title('Square root-transformed distribution')\n",
    "axes[2].hist(housing_df[f'{feature}_sqrt'], bins=50, edgecolor='black', color='grey')\n",
    "axes[2].set_xlabel(f'{feature}_sqrt')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "axes[3].set_title('Power-transformed distribution')\n",
    "axes[3].hist(housing_df[f'{feature}_power'], bins=50, edgecolor='black', color='grey')\n",
    "axes[3].set_xlabel(f'{feature}_power')\n",
    "axes[3].set_ylabel('Frequency')\n",
    "\n",
    "axes[4].set_title('Quantile transform (uniform)')\n",
    "axes[4].hist(housing_df[f'{feature}_quantile_uniform'], bins=50, edgecolor='black', color='grey')\n",
    "axes[4].set_xlabel('Uniform [0, 1]')\n",
    "axes[4].set_ylabel('Frequency')\n",
    "\n",
    "axes[5].set_xlabel('Normal distribution')\n",
    "axes[5].set_title('Quantile transform (normal)')\n",
    "axes[5].hist(housing_df[f'{feature}_quantile_normal'], bins=50, edgecolor='black', color='grey')\n",
    "axes[5].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e60c41",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform AveBedrms\n",
    "feature = 'AveBedrms'\n",
    "\n",
    "# Apply log transformation (using log1p to handle zeros)\n",
    "housing_df[f'{feature}_log'] = np.log1p(housing_df[feature])\n",
    "# Apply square root transformation\n",
    "housing_df[f'{feature}_sqrt'] = np.sqrt(housing_df[feature])\n",
    "# Apply power transformation using Yeo-Johnson (handles positive and negative values)\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "# Fit and transform\n",
    "housing_df[f'{feature}_power'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to uniform distribution\n",
    "transformer = QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "housing_df[f'{feature}_quantile_uniform'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to normal distribution\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "housing_df[f'{feature}_quantile_normal'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 4))\n",
    "\n",
    "axes[0].set_title('Original distribution')\n",
    "axes[0].hist(housing_df[feature], bins=50, edgecolor='black', color='grey')\n",
    "axes[0].set_xlabel(feature)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].set_title('Log-transformed distribution')\n",
    "axes[1].hist(housing_df[f'{feature}_log'], bins=50, edgecolor='black', color='grey')\n",
    "axes[1].set_xlabel(f'{feature}_log')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].set_title('Square root-transformed distribution')\n",
    "axes[2].hist(housing_df[f'{feature}_sqrt'], bins=50, edgecolor='black', color='grey')\n",
    "axes[2].set_xlabel(f'{feature}_sqrt')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "axes[3].set_title('Power-transformed distribution')\n",
    "axes[3].hist(housing_df[f'{feature}_power'], bins=50, edgecolor='black', color='grey')\n",
    "axes[3].set_xlabel(f'{feature}_power')\n",
    "axes[3].set_ylabel('Frequency')\n",
    "\n",
    "axes[4].set_title('Quantile transform (uniform)')\n",
    "axes[4].hist(housing_df[f'{feature}_quantile_uniform'], bins=50, edgecolor='black', color='grey')\n",
    "axes[4].set_xlabel('Uniform [0, 1]')\n",
    "axes[4].set_ylabel('Frequency')\n",
    "\n",
    "axes[5].set_xlabel('Normal distribution')\n",
    "axes[5].set_title('Quantile transform (normal)')\n",
    "axes[5].hist(housing_df[f'{feature}_quantile_normal'], bins=50, edgecolor='black', color='grey')\n",
    "axes[5].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008187c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform AveRooms\n",
    "feature = 'AveRooms'\n",
    "\n",
    "# Apply log transformation (using log1p to handle zeros)\n",
    "housing_df[f'{feature}_log'] = np.log1p(housing_df[feature])\n",
    "# Apply square root transformation\n",
    "housing_df[f'{feature}_sqrt'] = np.sqrt(housing_df[feature])\n",
    "# Apply power transformation using Yeo-Johnson (handles positive and negative values)\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "# Fit and transform\n",
    "housing_df[f'{feature}_power'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to uniform distribution\n",
    "transformer = QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "housing_df[f'{feature}_quantile_uniform'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to normal distribution\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "housing_df[f'{feature}_quantile_normal'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 4))\n",
    "\n",
    "axes[0].set_title('Original distribution')\n",
    "axes[0].hist(housing_df[feature], bins=50, edgecolor='black', color='grey')\n",
    "axes[0].set_xlabel(feature)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].set_title('Log-transformed distribution')\n",
    "axes[1].hist(housing_df[f'{feature}_log'], bins=50, edgecolor='black', color='grey')\n",
    "axes[1].set_xlabel(f'{feature}_log')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].set_title('Square root-transformed distribution')\n",
    "axes[2].hist(housing_df[f'{feature}_sqrt'], bins=50, edgecolor='black', color='grey')\n",
    "axes[2].set_xlabel(f'{feature}_sqrt')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "axes[3].set_title('Power-transformed distribution')\n",
    "axes[3].hist(housing_df[f'{feature}_power'], bins=50, edgecolor='black', color='grey')\n",
    "axes[3].set_xlabel(f'{feature}_power')\n",
    "axes[3].set_ylabel('Frequency')\n",
    "\n",
    "axes[4].set_title('Quantile transform (uniform)')\n",
    "axes[4].hist(housing_df[f'{feature}_quantile_uniform'], bins=50, edgecolor='black', color='grey')\n",
    "axes[4].set_xlabel('Uniform [0, 1]')\n",
    "axes[4].set_ylabel('Frequency')\n",
    "\n",
    "axes[5].set_xlabel('Normal distribution')\n",
    "axes[5].set_title('Quantile transform (normal)')\n",
    "axes[5].hist(housing_df[f'{feature}_quantile_normal'], bins=50, edgecolor='black', color='grey')\n",
    "axes[5].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1635b482",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform Population\n",
    "feature = 'Population'\n",
    "\n",
    "# Apply log transformation (using log1p to handle zeros)\n",
    "housing_df[f'{feature}_log'] = np.log1p(housing_df[feature])\n",
    "# Apply square root transformation\n",
    "housing_df[f'{feature}_sqrt'] = np.sqrt(housing_df[feature])\n",
    "# Apply power transformation using Yeo-Johnson (handles positive and negative values)\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "# Fit and transform\n",
    "housing_df[f'{feature}_power'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to uniform distribution\n",
    "transformer = QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "housing_df[f'{feature}_quantile_uniform'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to normal distribution\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "housing_df[f'{feature}_quantile_normal'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 4))\n",
    "\n",
    "axes[0].set_title('Original distribution')\n",
    "axes[0].hist(housing_df[feature], bins=50, edgecolor='black', color='grey')\n",
    "axes[0].set_xlabel(feature)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].set_title('Log-transformed distribution')\n",
    "axes[1].hist(housing_df[f'{feature}_log'], bins=50, edgecolor='black', color='grey')\n",
    "axes[1].set_xlabel(f'{feature}_log')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].set_title('Square root-transformed distribution')\n",
    "axes[2].hist(housing_df[f'{feature}_sqrt'], bins=50, edgecolor='black', color='grey')\n",
    "axes[2].set_xlabel(f'{feature}_sqrt')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "axes[3].set_title('Power-transformed distribution')\n",
    "axes[3].hist(housing_df[f'{feature}_power'], bins=50, edgecolor='black', color='grey')\n",
    "axes[3].set_xlabel(f'{feature}_power')\n",
    "axes[3].set_ylabel('Frequency')\n",
    "\n",
    "axes[4].set_title('Quantile transform (uniform)')\n",
    "axes[4].hist(housing_df[f'{feature}_quantile_uniform'], bins=50, edgecolor='black', color='grey')\n",
    "axes[4].set_xlabel('Uniform [0, 1]')\n",
    "axes[4].set_ylabel('Frequency')\n",
    "\n",
    "axes[5].set_xlabel('Normal distribution')\n",
    "axes[5].set_title('Quantile transform (normal)')\n",
    "axes[5].hist(housing_df[f'{feature}_quantile_normal'], bins=50, edgecolor='black', color='grey')\n",
    "axes[5].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12ec73",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform MedInc\n",
    "feature = 'MedInc'\n",
    "\n",
    "# Apply log transformation (using log1p to handle zeros)\n",
    "housing_df[f'{feature}_log'] = np.log1p(housing_df[feature])\n",
    "# Apply square root transformation\n",
    "housing_df[f'{feature}_sqrt'] = np.sqrt(housing_df[feature])\n",
    "# Apply power transformation using Yeo-Johnson (handles positive and negative values)\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "# Fit and transform\n",
    "housing_df[f'{feature}_power'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to uniform distribution\n",
    "transformer = QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "housing_df[f'{feature}_quantile_uniform'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to normal distribution\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "housing_df[f'{feature}_quantile_normal'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 4))\n",
    "\n",
    "axes[0].set_title('Original distribution')\n",
    "axes[0].hist(housing_df[feature], bins=50, edgecolor='black', color='grey')\n",
    "axes[0].set_xlabel(feature)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].set_title('Log-transformed distribution')\n",
    "axes[1].hist(housing_df[f'{feature}_log'], bins=50, edgecolor='black', color='grey')\n",
    "axes[1].set_xlabel(f'{feature}_log')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].set_title('Square root-transformed distribution')\n",
    "axes[2].hist(housing_df[f'{feature}_sqrt'], bins=50, edgecolor='black', color='grey')\n",
    "axes[2].set_xlabel(f'{feature}_sqrt')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "axes[3].set_title('Power-transformed distribution')\n",
    "axes[3].hist(housing_df[f'{feature}_power'], bins=50, edgecolor='black', color='grey')\n",
    "axes[3].set_xlabel(f'{feature}_power')\n",
    "axes[3].set_ylabel('Frequency')\n",
    "\n",
    "axes[4].set_title('Quantile transform (uniform)')\n",
    "axes[4].hist(housing_df[f'{feature}_quantile_uniform'], bins=50, edgecolor='black', color='grey')\n",
    "axes[4].set_xlabel('Uniform [0, 1]')\n",
    "axes[4].set_ylabel('Frequency')\n",
    "\n",
    "axes[5].set_xlabel('Normal distribution')\n",
    "axes[5].set_title('Quantile transform (normal)')\n",
    "axes[5].hist(housing_df[f'{feature}_quantile_normal'], bins=50, edgecolor='black', color='grey')\n",
    "axes[5].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034da79c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform MedHouseVal\n",
    "feature = 'MedHouseVal'\n",
    "\n",
    "# Apply log transformation (using log1p to handle zeros)\n",
    "housing_df[f'{feature}_log'] = np.log1p(housing_df[feature])\n",
    "# Apply square root transformation\n",
    "housing_df[f'{feature}_sqrt'] = np.sqrt(housing_df[feature])\n",
    "# Apply power transformation using Yeo-Johnson (handles positive and negative values)\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "# Fit and transform\n",
    "housing_df[f'{feature}_power'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to uniform distribution\n",
    "transformer = QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "housing_df[f'{feature}_quantile_uniform'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Apply quantile transformation to normal distribution\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "housing_df[f'{feature}_quantile_normal'] = transformer.fit_transform(housing_df[[feature]])\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 4))\n",
    "\n",
    "axes[0].set_title('Original distribution')\n",
    "axes[0].hist(housing_df[feature], bins=50, edgecolor='black', color='grey')\n",
    "axes[0].set_xlabel(feature)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].set_title('Log-transformed distribution')\n",
    "axes[1].hist(housing_df[f'{feature}_log'], bins=50, edgecolor='black', color='grey')\n",
    "axes[1].set_xlabel(f'{feature}_log')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].set_title('Square root-transformed distribution')\n",
    "axes[2].hist(housing_df[f'{feature}_sqrt'], bins=50, edgecolor='black', color='grey')\n",
    "axes[2].set_xlabel(f'{feature}_sqrt')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "axes[3].set_title('Power-transformed distribution')\n",
    "axes[3].hist(housing_df[f'{feature}_power'], bins=50, edgecolor='black', color='grey')\n",
    "axes[3].set_xlabel(f'{feature}_power')\n",
    "axes[3].set_ylabel('Frequency')\n",
    "\n",
    "axes[4].set_title('Quantile transform (uniform)')\n",
    "axes[4].hist(housing_df[f'{feature}_quantile_uniform'], bins=50, edgecolor='black', color='grey')\n",
    "axes[4].set_xlabel('Uniform [0, 1]')\n",
    "axes[4].set_ylabel('Frequency')\n",
    "\n",
    "axes[5].set_xlabel('Normal distribution')\n",
    "axes[5].set_title('Quantile transform (normal)')\n",
    "axes[5].hist(housing_df[f'{feature}_quantile_normal'], bins=50, edgecolor='black', color='grey')\n",
    "axes[5].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "housing_df.info()\n",
    "column_list = housing_df.columns.tolist()\n",
    "print(column_list)\n",
    "housing_df.drop([#'AveOccup_log',\n",
    "                 'AveOccup_sqrt', \n",
    "                 'AveOccup_power', \n",
    "                 'AveOccup_quantile_uniform',\n",
    "                 'AveOccup_quantile_normal',\n",
    "                 #'AveBedrms_log',\n",
    "                 'AveBedrms_sqrt', \n",
    "                 'AveBedrms_power', \n",
    "                 'AveBedrms_quantile_uniform',\n",
    "                 'AveBedrms_quantile_normal',\n",
    "                 #'AveRooms_log',\n",
    "                 'AveRooms_sqrt', \n",
    "                 'AveRooms_power', \n",
    "                 'AveRooms_quantile_uniform',\n",
    "                 'AveRooms_quantile_normal',\n",
    "                 #'Population_log',\n",
    "                 'Population_sqrt', \n",
    "                 'Population_power', \n",
    "                 'Population_quantile_uniform',\n",
    "                 'Population_quantile_normal',\n",
    "                 #'MedInc_log',\n",
    "                 'MedInc_sqrt', \n",
    "                 'MedInc_power', \n",
    "                 'MedInc_quantile_uniform', \n",
    "                 'MedInc_quantile_normal',\n",
    "                 #'MedHouseVal_log',\n",
    "                 'MedHouseVal_sqrt',\n",
    "                 'MedHouseVal_power', \n",
    "                 'MedHouseVal_quantile_uniform',\n",
    "                 'MedHouseVal_quantile_normal'], axis=1, inplace=True)\n",
    "\n",
    "housing_df.info()\n",
    "\n",
    "housing_df.drop(['MedInc',  'AveRooms', 'AveBedrms', 'Population', 'AveOccup','MedHouseVal'], axis=1, inplace=True)\n",
    "\n",
    "housing_df.rename(columns={'MedInc_log':'MedInc', 'AveRooms_log': 'AveRooms','AveOccup_log':'AveOccup','AveBedrms_log':'AveBedrms', 'Population_log':'Population','MedHouseVal_log':'MedHouseVal'}, inplace=True)\n",
    "housing_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59bc77d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create bins/categories from continuous variables\n",
    "\n",
    "# Bin HouseAge into age groups\n",
    "housing_df['HouseAge_binned'] = pd.cut(housing_df['HouseAge'], \n",
    "                                        bins=[0, 10, 20, 30, 40, 52], \n",
    "                                        labels=['1-10', '11-20', '21-30', '31-40', '41-52'])\n",
    "\n",
    "# Bin Latitude into geographic regions\n",
    "#housing_df['Latitude_binned'] = pd.cut(housing_df['Latitude'], bins=5, labels=['South', 'South-Central', 'Central', 'North-Central', 'North'])\n",
    "\n",
    "# Bin Longitude into geographic regions\n",
    "#housing_df['Longitude_binned'] = pd.cut(housing_df['Longitude'], bins=5, labels=['Far West', 'West', 'Central', 'East', 'Far East'])\n",
    "\n",
    "# Bin MedInc into income brackets\n",
    "#housing_df['MedInc_binned'] = pd.cut(housing_df['MedInc'], bins=[0, 2, 4, 6, 8, 16], labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Convert binned columns to numeric codes for model compatibility\n",
    "housing_df['HouseAge_binned_encoded'] = pd.Categorical(housing_df['HouseAge_binned']).codes\n",
    "#housing_df['Latitude_binned_encoded'] = pd.Categorical(housing_df['Latitude_binned']).codes\n",
    "#housing_df['Longitude_binned_encoded'] = pd.Categorical(housing_df['Longitude_binned']).codes\n",
    "#housing_df['MedInc_binned_encoded'] = pd.Categorical(housing_df['MedInc_binned']).codes\n",
    "\n",
    "# Visualize the binned features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# HouseAge binned\n",
    "axes[0, 0].bar(housing_df['HouseAge_binned'].value_counts().index, \n",
    "               housing_df['HouseAge_binned'].value_counts().values, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('HouseAge Distribution (Binned)')\n",
    "axes[0, 0].set_xlabel('Age Group')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Latitude binned\n",
    "#axes[0, 1].bar(housing_df['Latitude_binned'].value_counts().index, \n",
    " #              housing_df['Latitude_binned'].value_counts().values, color='lightcoral', edgecolor='black')\n",
    "#axes[0, 1].set_title('Latitude Distribution (Binned)')\n",
    "#axes[0, 1].set_xlabel('Region')\n",
    "#axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Longitude binned\n",
    "#axes[1, 0].bar(housing_df['Longitude_binned'].value_counts().index, \n",
    " #             housing_df['Longitude_binned'].value_counts().values, color='lightgreen', edgecolor='black')\n",
    "#axes[1, 0].set_title('Longitude Distribution (Binned)')\n",
    "#axes[1, 0].set_xlabel('Region')\n",
    "#axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# MedInc binned\n",
    "#axes[1, 1].bar(housing_df['MedInc_binned'].value_counts().index, \n",
    "            #   housing_df['MedInc_binned'].value_counts().values, color='gold', edgecolor='black')\n",
    "#axes[1, 1].set_title('MedInc Distribution (Binned)')\n",
    "#axes[1, 1].set_xlabel('Income Bracket')\n",
    "#axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the binned dataframe info\n",
    "print(\"\\nBinned features added:\")\n",
    "#print(housing_df[['HouseAge_binned', 'Latitude_binned', 'Longitude_binned', 'MedInc_binned']].head(10))\n",
    "print(\"\\nEncoded binned features:\")\n",
    "#print(housing_df[['HouseAge_binned_encoded', 'Latitude_binned_encoded', 'Longitude_binned_encoded', 'MedInc_binned_encoded']].head(10))\n",
    "\n",
    "# Replace original features with binned encoded versions\n",
    "housing_df.drop(['HouseAge', \n",
    "                 #'Latitude', \n",
    "                 #'Longitude', \n",
    "                 #'MedInc', \n",
    "                 'HouseAge_binned',\n",
    "                 #'Latitude_binned', \n",
    "                 #'Longitude_binned'\n",
    "                 #'MedInc_binned'\n",
    "                 ], \n",
    "                axis=1, inplace=True)\n",
    "\n",
    "housing_df.rename(columns={\n",
    "    'HouseAge_binned_encoded': 'HouseAge',\n",
    "    #'Latitude_binned_encoded': 'Latitude',\n",
    "    #'Longitude_binned_encoded': 'Longitude'\n",
    "    #'MedInc_binned_encoded': 'MedInc'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Original features replaced with binned encoded versions:\")\n",
    "print(housing_df.head())\n",
    "print(\"\\nDataframe info:\")\n",
    "print(housing_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89654603",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "\n",
    "# Reload the original features to create interactions\n",
    "# (We need the original Population and AveRooms values for interactions)\n",
    "original_housing_df_temp = pd.read_csv('https://gperdrizet.github.io/FSA_devops/assets/data/unit2/california_housing.csv')\n",
    "\n",
    "# Feature 1: Rooms per household (total rooms / population)\n",
    "# This indicates room density relative to population\n",
    "housing_df['rooms_per_household'] = original_housing_df_temp['AveRooms'] / original_housing_df_temp['Population']\n",
    "\n",
    "# Feature 2: Bedrooms per room (bedrooms / rooms)\n",
    "# This indicates the proportion of rooms that are bedrooms\n",
    "housing_df['bedrooms_per_room'] = original_housing_df_temp['AveBedrms'] / original_housing_df_temp['AveRooms']\n",
    "\n",
    "# Feature 3: Occupancy efficiency (population / occupancy)\n",
    "# This indicates how many people per occupied unit\n",
    "housing_df['occupancy_efficiency'] = original_housing_df_temp['Population'] / original_housing_df_temp['AveOccup']\n",
    "\n",
    "# Feature 4: Income × Population interaction (scaled income with population size)\n",
    "# Higher income areas with more population might have different value patterns\n",
    "housing_df['income_population_interaction'] = original_housing_df_temp['MedInc'] * original_housing_df_temp['Population']\n",
    "\n",
    "# Feature 5: Room quality index (rooms + bedrooms interaction normalized)\n",
    "# Combines room count and bedroom count as a proxy for housing quality\n",
    "housing_df['room_quality_index'] = (original_housing_df_temp['AveRooms'] * original_housing_df_temp['AveBedrms']) / (original_housing_df_temp['AveOccup'] + 1)\n",
    "\n",
    "# Visualize the new interaction features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# rooms_per_household\n",
    "sns.histplot(housing_df['rooms_per_household'], kde=True, color='skyblue', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Rooms per Household')\n",
    "axes[0, 0].set_xlabel('Rooms per Household')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# bedrooms_per_room\n",
    "sns.histplot(housing_df['bedrooms_per_room'], kde=True, color='lightcoral', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Bedrooms per Room')\n",
    "axes[0, 1].set_xlabel('Bedrooms per Room')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# occupancy_efficiency\n",
    "sns.histplot(housing_df['occupancy_efficiency'], kde=True, color='lightgreen', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Occupancy Efficiency')\n",
    "axes[0, 2].set_xlabel('People per Occupied Unit')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# income_population_interaction\n",
    "sns.histplot(housing_df['income_population_interaction'], kde=True, color='gold', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Income × Population Interaction')\n",
    "axes[1, 0].set_xlabel('Interaction Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# room_quality_index\n",
    "sns.histplot(housing_df['room_quality_index'], kde=True, color='plum', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Room Quality Index')\n",
    "axes[1, 1].set_xlabel('Quality Index')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Correlation with target\n",
    "ax = axes[1, 2]\n",
    "interaction_cols = ['rooms_per_household', 'bedrooms_per_room', 'occupancy_efficiency', \n",
    "                   'income_population_interaction', 'room_quality_index']\n",
    "correlations = [housing_df[col].corr(housing_df['MedHouseVal']) for col in interaction_cols]\n",
    "ax.barh(interaction_cols, correlations, color='steelblue')\n",
    "ax.set_title('Interaction Features Correlation with MedHouseVal')\n",
    "ax.set_xlabel('Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interaction features created:\")\n",
    "print(f\"  - rooms_per_household: AveRooms / Population\")\n",
    "print(f\"  - bedrooms_per_room: AveBedrms / AveRooms\")\n",
    "print(f\"  - occupancy_efficiency: Population / AveOccup\")\n",
    "print(f\"  - income_population_interaction: MedInc × Population\")\n",
    "print(f\"  - room_quality_index: (AveRooms × AveBedrms) / (AveOccup + 1)\")\n",
    "print(\"\\nFirst few rows of interaction features:\")\n",
    "print(housing_df[interaction_cols].head(10))\n",
    "print(\"\\nDataframe info:\")\n",
    "print(housing_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110a0a2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Task 3: Apply your second feature engineering technique\n",
    "\n",
    "**Example approaches**:\n",
    "- Scale features to similar ranges\n",
    "- Encode any categorical variables you created\n",
    "- Create aggregate statistics by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04645938",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Scale features to similar ranges\n",
    "\n",
    "# Separate target variable from features\n",
    "X = housing_df.drop('MedHouseVal', axis=1)\n",
    "y = housing_df['MedHouseVal']\n",
    "\n",
    "# Create StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert scaled features back to DataFrame to maintain column names\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Add target variable back\n",
    "housing_df_scaled = X_scaled_df.copy()\n",
    "housing_df_scaled['MedHouseVal'] = y.values\n",
    "\n",
    "# Display statistics before and after scaling\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE SCALING COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nBefore Scaling (Original Features):\")\n",
    "print(housing_df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nAfter Scaling (Standardized Features):\")\n",
    "print(housing_df_scaled.describe())\n",
    "\n",
    "# Visualize the difference between original and scaled features\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Select a few key features to visualize\n",
    "key_features = ['MedInc', 'Population', 'rooms_per_household', 'room_quality_index']\n",
    "\n",
    "# Plot original features\n",
    "for feature in key_features:\n",
    "    axes[0].hist(housing_df[feature], alpha=0.6, label=feature, bins=30, edgecolor='black')\n",
    "axes[0].set_title('Original Features (Different Scales)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot scaled features\n",
    "for feature in key_features:\n",
    "    axes[1].hist(housing_df_scaled[feature], alpha=0.6, label=feature, bins=30, edgecolor='black')\n",
    "axes[1].set_title('Scaled Features (Standardized - Mean=0, Std=1)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Standardized Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update housing_df with scaled features\n",
    "housing_df = housing_df_scaled\n",
    "\n",
    "print(\"\\nScaling Summary:\")\n",
    "print(f\"  - Scaling method: StandardScaler (z-score normalization)\")\n",
    "print(f\"  - Mean of scaled features: ~0\")\n",
    "print(f\"  - Standard deviation of scaled features: ~1\")\n",
    "print(f\"  - Shape of engineered dataset: {housing_df.shape}\")\n",
    "print(f\"\\nScaled dataframe info:\")\n",
    "print(housing_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1991a6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode any categorical variables you created\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CATEGORICAL VARIABLE ENCODING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if we have any categorical features in the dataset\n",
    "print(\"\\nDataset Info (before encoding):\")\n",
    "print(housing_df.dtypes)\n",
    "\n",
    "# Since we've already encoded HouseAge during binning (it's now numeric),\n",
    "# let's verify it and document the encoding scheme\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENCODING SCHEME FOR BINNED FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nHouseAge Binning Encoding:\")\n",
    "print(\"  Original bins: [1-10, 11-20, 21-30, 31-40, 41-52]\")\n",
    "print(\"  Encoded values: [0, 1, 2, 3, 4]\")\n",
    "print(\"  (Ordinal encoding preserves the natural ordering of age groups)\")\n",
    "\n",
    "# Display sample of encoded features\n",
    "print(\"\\nSample of encoded HouseAge values:\")\n",
    "original_housing_df_temp = pd.read_csv('https://gperdrizet.github.io/FSA_devops/assets/data/unit2/california_housing.csv')\n",
    "house_age_bins = pd.cut(original_housing_df_temp['HouseAge'], \n",
    "                        bins=[0, 10, 20, 30, 40, 52], \n",
    "                        labels=['1-10', '11-20', '21-30', '31-40', '41-52'])\n",
    "house_age_encoded = pd.Categorical(house_age_bins).codes\n",
    "\n",
    "print(\"\\nOriginal HouseAge -> Binned -> Encoded\")\n",
    "for i in range(10):\n",
    "    print(f\"  {original_housing_df_temp['HouseAge'].iloc[i]:5.0f} -> {house_age_bins.iloc[i]:>6} -> {house_age_encoded[i]}\")\n",
    "\n",
    "print(\"\\nEncoding Summary:\")\n",
    "print(\"  - Encoding Type: Ordinal Encoding (for binned HouseAge)\")\n",
    "print(\"  - Reason: HouseAge bins have a natural ordering (older → newer)\")\n",
    "print(\"  - All other features: Already numeric (no encoding needed)\")\n",
    "print(f\"  - Total features in engineered dataset: {housing_df.shape[1]}\")\n",
    "\n",
    "print(\"\\nFinal Engineered Dataset:\")\n",
    "print(housing_df.head(10))\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"  Shape: {housing_df.shape}\")\n",
    "print(f\"  No missing values: {housing_df.isnull().sum().sum() == 0}\")\n",
    "print(f\"  All features are numeric: {housing_df.select_dtypes(include=[np.number]).shape[1] == housing_df.shape[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80af698",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create agg statistics by groups\n",
    "# Reload original data to create groups\n",
    "original_housing_df_temp = pd.read_csv('https://gperdrizet.github.io/FSA_devops/assets/data/unit2/california_housing.csv')\n",
    "\n",
    "# Create income groups based on MedInc quartiles\n",
    "income_groups = pd.qcut(original_housing_df_temp['MedInc'], q=4, labels=['Low Income', 'Lower-Mid Income', 'Upper-Mid Income', 'High Income'])\n",
    "\n",
    "# Create population density groups\n",
    "pop_groups = pd.qcut(original_housing_df_temp['Population'], q=4, labels=['Sparse', 'Low Density', 'Medium Density', 'High Density'])\n",
    "\n",
    "# Create house age groups\n",
    "age_groups = pd.cut(original_housing_df_temp['HouseAge'], \n",
    "                     bins=[0, 10, 20, 30, 40, 52], \n",
    "                     labels=['Very New (1-10)', 'New (11-20)', 'Medium (21-30)', 'Old (31-40)', 'Very Old (41-52)'])\n",
    "\n",
    "# Add groups to original dataframe temporarily\n",
    "original_with_groups = original_housing_df_temp.copy()\n",
    "original_with_groups['income_group'] = income_groups\n",
    "original_with_groups['population_group'] = pop_groups\n",
    "original_with_groups['age_group'] = age_groups\n",
    "\n",
    "# 1. Group by Income Levels\n",
    "print(\"\\n1. MEDIAN HOUSE VALUE BY INCOME GROUP\")\n",
    "print(\"-\" * 70)\n",
    "income_agg = original_with_groups.groupby('income_group')['MedHouseVal'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "print(income_agg)\n",
    "print(f\"\\nInsight: Higher income areas have significantly higher house values\")\n",
    "\n",
    "# 2. Group by Population Density\n",
    "print(\"\\n2. MEDIAN HOUSE VALUE BY POPULATION DENSITY\")\n",
    "print(\"-\" * 70)\n",
    "pop_agg = original_with_groups.groupby('population_group')['MedHouseVal'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "print(pop_agg)\n",
    "print(f\"\\nInsight: Population density patterns show impact on house values\")\n",
    "\n",
    "# 3. Group by House Age\n",
    "print(\"\\n3. MEDIAN HOUSE VALUE BY HOUSE AGE\")\n",
    "print(\"-\" * 70)\n",
    "age_agg = original_with_groups.groupby('age_group')['MedHouseVal'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "print(age_agg)\n",
    "print(f\"\\nInsight: Newer houses tend to have higher values\")\n",
    "\n",
    "# 4. Multi-level grouping: Income x Age\n",
    "print(\"\\n4. MEDIAN HOUSE VALUE BY INCOME AND AGE GROUP\")\n",
    "print(\"-\" * 70)\n",
    "income_age_agg = original_with_groups.groupby(['income_group', 'age_group'])['MedHouseVal'].agg(['count', 'mean'])\n",
    "print(income_age_agg)\n",
    "\n",
    "# 5. Create aggregate features from groups\n",
    "print(\"\\n5. CREATING GROUP-BASED AGGREGATE FEATURES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Mean house value by income group\n",
    "income_mean = original_with_groups.groupby('income_group')['MedHouseVal'].transform('mean')\n",
    "\n",
    "# Mean house value by population density group\n",
    "pop_mean = original_with_groups.groupby('population_group')['MedHouseVal'].transform('mean')\n",
    "\n",
    "# Mean house value by age group\n",
    "age_mean = original_with_groups.groupby('age_group')['MedHouseVal'].transform('mean')\n",
    "\n",
    "# Add these as new features to housing_df\n",
    "housing_df['income_group_mean'] = income_mean.values\n",
    "housing_df['population_group_mean'] = pop_mean.values\n",
    "housing_df['age_group_mean'] = age_mean.values\n",
    "\n",
    "print(\"New aggregate features added:\")\n",
    "print(\"  - income_group_mean: Mean house value for each income group\")\n",
    "print(\"  - population_group_mean: Mean house value for each population density group\")\n",
    "print(\"  - age_group_mean: Mean house value for each age group\")\n",
    "\n",
    "# 6. Visualize aggregate statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Income group plot\n",
    "income_agg['mean'].plot(kind='bar', ax=axes[0, 0], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Mean House Value by Income Group', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Income Group')\n",
    "axes[0, 0].set_ylabel('Mean House Value')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Population density plot\n",
    "pop_agg['mean'].plot(kind='bar', ax=axes[0, 1], color='coral', edgecolor='black')\n",
    "axes[0, 1].set_title('Mean House Value by Population Density', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Population Density')\n",
    "axes[0, 1].set_ylabel('Mean House Value')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Age group plot\n",
    "age_agg['mean'].plot(kind='bar', ax=axes[1, 0], color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('Mean House Value by House Age', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('House Age Group')\n",
    "axes[1, 0].set_ylabel('Mean House Value')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sample distribution of group means\n",
    "axes[1, 1].hist([income_mean, pop_mean, age_mean], label=['Income Group', 'Population Group', 'Age Group'], bins=20, alpha=0.6, edgecolor='black')\n",
    "axes[1, 1].set_title('Distribution of Aggregate Group Means', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Mean House Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nUpdated engineered dataset with aggregate features:\")\n",
    "print(f\"  Shape: {housing_df.shape}\")\n",
    "print(f\"  New columns: {housing_df.shape[1] - 18} aggregate features added\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(housing_df[['income_group_mean', 'population_group_mean', 'age_group_mean', 'MedHouseVal']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90f141",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## (Optional) Additional feature engineering\n",
    "\n",
    "Add more techniques if you'd like to experiment further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee0a0c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11834ffc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "Now we'll compare model performance on the original dataset versus your engineered dataset.\n",
    "\n",
    "### Evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a836f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save a copy of the engineered dataframe\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#housing_df.to_csv('housing_df.csv', index=False)\n",
    "# Create output directory if it doesn't exist\n",
    "#output_directory = 'data/outputs'\n",
    "Path('data/outputs').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save a copy of the engineered dataframe\n",
    "housing_df.to_csv('data/outputs/housing_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691bfe0c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Evaluate on original dataset\n",
    "scores_original = cross_val_score(\n",
    "    model,\n",
    "    original_housing_df.drop('MedHouseVal', axis=1),\n",
    "    original_housing_df['MedHouseVal'],\n",
    "    cv=10,\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "# Evaluate on engineered dataset\n",
    "scores_engineered = cross_val_score(\n",
    "    model,\n",
    "    housing_df.drop('MedHouseVal', axis=1),\n",
    "    housing_df['MedHouseVal'],\n",
    "    cv=10,\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "engineered_mean = scores_engineered.mean()\n",
    "original_mean = scores_original.mean()\n",
    "mean_improvement = (engineered_mean - original_mean) / original_mean\n",
    "\n",
    "print(f'\\nMean improvement: {mean_improvement:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017b15b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Visualize model performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff7949",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_model = LinearRegression()\n",
    "original_model.fit(original_housing_df.drop('MedHouseVal', axis=1), original_housing_df['MedHouseVal'])\n",
    "original_predictions = original_model.predict(original_housing_df.drop('MedHouseVal', axis=1))\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(housing_df.drop('MedHouseVal', axis=1), housing_df['MedHouseVal'])\n",
    "predictions = model.predict(housing_df.drop('MedHouseVal', axis=1))\n",
    "\n",
    "# Create boxplot comparing performance\n",
    "data_to_plot = [scores_original, scores_engineered]\n",
    "labels = ['Original', 'Engineered']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(9,4.5))\n",
    "\n",
    "fig.suptitle(f'Model performance comparison\\nmean improvement: {mean_improvement:.2f}%')\n",
    "\n",
    "axs[0].set_title('Cross validation R² scores')\n",
    "axs[0].boxplot(data_to_plot, tick_labels=labels)\n",
    "axs[0].set_xlabel('Dataset')\n",
    "axs[0].set_ylabel('R² score')\n",
    "\n",
    "axs[1].set_title('Predictions vs true values')\n",
    "axs[1].plot(\n",
    "    original_housing_df['MedHouseVal'], original_predictions,\n",
    "    'o', markersize=1, label='Original', alpha=0.25\n",
    ")\n",
    "\n",
    "axs[1].plot(\n",
    "    housing_df['MedHouseVal'], predictions,\n",
    "    'o', markersize=1, label='Engineered', alpha=0.25\n",
    ")\n",
    "\n",
    "axs[1].set_xlabel('True Values')\n",
    "axs[1].set_ylabel('Predictions')\n",
    "\n",
    "leg = axs[1].legend(loc='upper left', markerscale=8, framealpha=1)\n",
    "\n",
    "for lh in leg.legend_handles: \n",
    "    lh.set_alpha(1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bc24e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Reflection\n",
    "\n",
    "**Questions to consider**:\n",
    "\n",
    "1. Which feature engineering techniques had the biggest impact on model performance?\n",
    "# Tranformations\n",
    "2. Did adding more features always improve performance, or did some hurt it?\n",
    "# Interaction features helped but only on few features\n",
    "3. How might you further improve the engineered dataset?\n",
    "#  May be get more accurat\n",
    "4. What trade-offs did you consider (e.g., interpretability vs performance, complexity vs gains)?\n",
    "#  Focused on the improving model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f57096",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Your reflection**:\n",
    "\n",
    "*Write your thoughts here...*\n",
    "\n",
    "\n",
    "\n",
    "* Remove low-correlation features for a cleaner model\n",
    "* Handle outliers to prevent predictions from being skewed\n",
    "* Add polynomial/interaction features to capture non-linear relationships"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.012553,
   "end_time": "2025-11-20T18:31:01.156912",
   "environment_variables": {},
   "exception": true,
   "input_path": "data/submissions/Pallavi_20251120_193059.ipynb",
   "output_path": "data/outputs/Pallavi_20251120_193059_20251120_193059_executed.ipynb",
   "parameters": {},
   "start_time": "2025-11-20T18:30:59.144359",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}