"""Scoring module for evaluating notebook outputs.

This module provides a customizable scoring function that evaluates
CSV files generated by executed notebooks. Modify this to match your 
specific assignment requirements.
"""

import os
import pandas as pd
from typing import Optional, Any
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score


class Scorer:
    """Handles scoring of notebook outputs based on CSV files."""

    def __init__(self, ground_truth_path: Optional[str] = None):
        """Initialize scorer.
        
        Args:
            ground_truth_path: Optional path to ground truth CSV file
        """
        self.ground_truth_path = ground_truth_path
        self.ground_truth = None

        if ground_truth_path and os.path.exists(ground_truth_path):
            self._load_ground_truth()

    def _load_ground_truth(self):
        """Load ground truth data from CSV file."""
        try:
            self.ground_truth = pd.read_csv(self.ground_truth_path)
        except Exception as e:
            print(f"Warning: Could not load ground truth: {e}")
            self.ground_truth = None

    def score_notebook(
        self,
        executed_notebook_path: str,
        output_data: Optional[Any] = None
    ) -> float:
        """Score an executed notebook based on its CSV output.
        
        This function looks for a CSV file in the same directory as the 
        executed notebook and scores it against ground truth.
        
        Args:
            executed_notebook_path: Path to the executed notebook
            output_data: Optional (not used, kept for compatibility)
            
        Returns:
            Score as a float
        """
        try:
            # Get the directory containing the notebook
            notebook_dir = os.path.dirname(executed_notebook_path)
            
            # If notebook_dir is empty (e.g., just a filename), use current directory
            if not notebook_dir:
                notebook_dir = '.'
            
            # Check if path is completely invalid
            if not os.path.exists(notebook_dir):
                print(f"ERROR: Notebook directory does not exist: {notebook_dir}")
                return 0.0
            
            # Also check the current working directory (where notebooks execute from)
            cwd = os.getcwd()
            
            # Exclude ground truth and other system CSVs
            exclude_files = ['california_housing.csv', 'housing_df.csv']
            
            # Look for CSV file in multiple locations
            csv_files = []
            
            # First, check the notebook's directory
            if os.path.exists(notebook_dir):
                csv_files = [
                    os.path.join(notebook_dir, f) 
                    for f in os.listdir(notebook_dir) 
                    if f.endswith('.csv') and f not in exclude_files
                ]
            
            # If not found, check the current working directory
            if not csv_files and os.path.exists(cwd):
                csv_files = [
                    os.path.join(cwd, f) 
                    for f in os.listdir(cwd) 
                    if f.endswith('.csv') and f not in exclude_files and 'data/' not in f
                ]
                if csv_files:
                    print(f"Found CSV file(s) in working directory: {cwd}")
            
            if not csv_files:
                print(f"ERROR: No CSV file found in notebook directory: {notebook_dir}")
                print(f"Also checked working directory: {cwd}")
                print(f"Files in notebook dir: {os.listdir(notebook_dir) if os.path.exists(notebook_dir) else 'N/A'}")
                print(f"CSV files in working dir (excluding system files): {[f for f in os.listdir(cwd) if f.endswith('.csv') and f not in exclude_files] if os.path.exists(cwd) else 'N/A'}")
                return 0.0
            
            # Use the first CSV file found (or implement custom logic to find the right one)
            csv_path = csv_files[0]
            print(f"Found CSV file: {csv_path}")
            
            # Read the submission CSV
            submission_df = pd.read_csv(csv_path)
            print(f"CSV loaded successfully. Shape: {submission_df.shape}, Columns: {list(submission_df.columns)}")
            
            # Score based on ground truth if available
            if self.ground_truth is not None:
                print("Using ground truth for scoring")
                score = self._score_against_ground_truth(submission_df)
            else:
                # Basic scoring if no ground truth available
                print("Using basic CSV validation for scoring")
                score = self._basic_csv_validation(submission_df)
            
            print(f"Final score: {score}")
            return score

        except Exception as e:
            print(f"ERROR in score_notebook: {type(e).__name__}: {str(e)}")
            import traceback
            traceback.print_exc()
            return 0.0

    def _score_against_ground_truth(self, submission_df: pd.DataFrame) -> float:
        """Score submission DataFrame against ground truth.
        
        Args:
            submission_df: DataFrame from submission CSV
            
        Returns:
            Score as a float
        """
        try:
            print(f"Ground truth shape: {self.ground_truth.shape}, columns: {list(self.ground_truth.columns)}")
            print(f"Submission shape: {submission_df.shape}, columns: {list(submission_df.columns)}")
            
            model = LinearRegression()

            # Evaluate on original dataset
            scores_original = cross_val_score(
                model,
                self.ground_truth.drop('MedHouseVal', axis=1),
                self.ground_truth['MedHouseVal'],
                cv=10,
                scoring='r2'
            )

            # Evaluate on engineered dataset
            scores_engineered = cross_val_score(
                model,
                submission_df.drop('MedHouseVal', axis=1),
                submission_df['MedHouseVal'],
                cv=10,
                scoring='r2'
            )

            engineered_mean = scores_engineered.mean()
            original_mean = scores_original.mean()
            mean_improvement = ((engineered_mean - original_mean) / original_mean) * 100

            print(f"Original Mean R2: {original_mean:.4f}, Engineered Mean R2: {engineered_mean:.4f}, Improvement: {mean_improvement:.2f}%")

            return mean_improvement
            
        except Exception as e:
            print(f"ERROR in _score_against_ground_truth: {type(e).__name__}: {str(e)}")
            import traceback
            traceback.print_exc()
            return 0.0

    def _basic_csv_validation(self, submission_df: pd.DataFrame) -> float:
        """Basic validation when no ground truth is available.
        
        Args:
            submission_df: DataFrame from submission CSV
            
        Returns:
            Score as a float
        """
        score = 0.0
        
        # Check if CSV has content
        if submission_df.empty:
            return 0.0
        
        # Award points for having data
        score += 50.0
        
        # Check for null values
        null_count = submission_df.isnull().sum().sum()
        if null_count == 0:
            score += 25.0
        
        # Basic format check - award remaining points
        score += 25.0
        
        return score

    def score_from_csv_path(self, csv_path: str) -> float:
        """Score a CSV file directly by path.
        
        Args:
            csv_path: Path to the CSV file
            
        Returns:
            Score as a float
        """
        try:
            submission_df = pd.read_csv(csv_path)
            
            if self.ground_truth is not None:
                return self._score_against_ground_truth(submission_df)
            else:
                return self._basic_csv_validation(submission_df)
                
        except Exception as e:
            return 0.0
