"""Scoring module for evaluating notebook outputs.

This module provides a customizable scoring function that evaluates
CSV files generated by executed notebooks. Modify this to match your 
specific assignment requirements.
"""

import os
import pandas as pd
from typing import Optional, Tuple, Any


class Scorer:
    """Handles scoring of notebook outputs based on CSV files."""

    def __init__(self, ground_truth_path: Optional[str] = None):
        """Initialize scorer.
        
        Args:
            ground_truth_path: Optional path to ground truth CSV file
        """
        self.ground_truth_path = ground_truth_path
        self.ground_truth = None

        if ground_truth_path and os.path.exists(ground_truth_path):
            self._load_ground_truth()

    def _load_ground_truth(self):
        """Load ground truth data from CSV file."""
        try:
            self.ground_truth = pd.read_csv(self.ground_truth_path)
        except Exception as e:
            print(f"Warning: Could not load ground truth: {e}")
            self.ground_truth = None

    def score_notebook(
        self,
        executed_notebook_path: str,
        output_data: Optional[Any] = None
    ) -> Tuple[float, str]:
        """Score an executed notebook based on its CSV output.
        
        This function looks for a CSV file in the same directory as the 
        executed notebook and scores it against ground truth.
        
        Args:
            executed_notebook_path: Path to the executed notebook
            output_data: Optional (not used, kept for compatibility)
            
        Returns:
            Tuple of (score, feedback_message)
        """
        try:
            # Get the directory containing the notebook
            notebook_dir = os.path.dirname(executed_notebook_path)
            
            # Look for CSV file in the output directory
            # Assume the CSV has a similar name to the notebook or is named 'output.csv'
            csv_files = [f for f in os.listdir(notebook_dir) if f.endswith('.csv')]
            
            if not csv_files:
                return 0.0, "No CSV file found in notebook output directory"
            
            # Use the first CSV file found (or implement custom logic to find the right one)
            csv_path = os.path.join(notebook_dir, csv_files[0])
            
            # Read the submission CSV
            submission_df = pd.read_csv(csv_path)
            
            # Score based on ground truth if available
            if self.ground_truth is not None:
                score, feedback = self._score_against_ground_truth(submission_df)
            else:
                # Basic scoring if no ground truth available
                score, feedback = self._basic_csv_validation(submission_df)
            
            return score, feedback

        except Exception as e:
            return 0.0, f"Error scoring notebook: {str(e)}"

    def _score_against_ground_truth(self, submission_df: pd.DataFrame) -> Tuple[float, str]:
        """Score submission DataFrame against ground truth.
        
        Args:
            submission_df: DataFrame from submission CSV
            
        Returns:
            Tuple of (score, feedback_message)
        """
        try:
            score = 0.0
            feedback_parts = []
            
            # Check if DataFrames have same shape
            if submission_df.shape != self.ground_truth.shape:
                return 0.0, f"CSV shape mismatch. Expected {self.ground_truth.shape}, got {submission_df.shape}"
            
            # Check if columns match
            if not all(submission_df.columns == self.ground_truth.columns):
                feedback_parts.append("Warning: Column names don't match exactly")
            
            # Calculate accuracy (percentage of matching values)
            # This is a simple example - customize based on your needs
            total_cells = submission_df.size
            matching_cells = (submission_df == self.ground_truth).sum().sum()
            accuracy = matching_cells / total_cells
            
            score = accuracy * 100  # Convert to 0-100 scale
            feedback_parts.append(f"Accuracy: {accuracy:.2%}")
            feedback_parts.append(f"Matching cells: {matching_cells}/{total_cells}")
            
            feedback = " | ".join(feedback_parts)
            return score, feedback
            
        except Exception as e:
            return 0.0, f"Error comparing with ground truth: {str(e)}"

    def _basic_csv_validation(self, submission_df: pd.DataFrame) -> Tuple[float, str]:
        """Basic validation when no ground truth is available.
        
        Args:
            submission_df: DataFrame from submission CSV
            
        Returns:
            Tuple of (score, feedback_message)
        """
        score = 0.0
        feedback_parts = []
        
        # Check if CSV has content
        if submission_df.empty:
            return 0.0, "CSV file is empty"
        
        # Award points for having data
        score += 50.0
        feedback_parts.append(f"CSV contains {len(submission_df)} rows and {len(submission_df.columns)} columns")
        
        # Check for null values
        null_count = submission_df.isnull().sum().sum()
        if null_count == 0:
            score += 25.0
            feedback_parts.append("No null values found")
        else:
            feedback_parts.append(f"Warning: {null_count} null values found")
        
        # Basic format check - award remaining points
        score += 25.0
        
        feedback = " | ".join(feedback_parts)
        return score, feedback

    def score_from_csv_path(self, csv_path: str) -> Tuple[float, str]:
        """Score a CSV file directly by path.
        
        Args:
            csv_path: Path to the CSV file
            
        Returns:
            Tuple of (score, feedback_message)
        """
        try:
            submission_df = pd.read_csv(csv_path)
            
            if self.ground_truth is not None:
                return self._score_against_ground_truth(submission_df)
            else:
                return self._basic_csv_validation(submission_df)
                
        except Exception as e:
            return 0.0, f"Error reading CSV: {str(e)}"
